##poll(Duration)这个版本修改了这样的设计，会把元数据获取也计入整个超时时间，从而避免了无效等待。
##ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
#  while (true) {
#  if(consumer != null)
#  {
#  // timeout 阻塞时间，从kafka中取出100毫秒的数据，有可能一次取出0到N条
#  List<Map<String,Object>> datas = new ArrayList<>();
#  ConsumerRecords<String, String> records = consumer.poll(100);
#  // 遍历
#for (ConsumerRecord<String, String> record : records) {
#  Map<String,Object> notifyDto = ( Map<String,Object> ) JsonUtils.jsonToMap(record.value());
#  datas.add(notifyDto);
#}
#  // 拿出结果
#  if(CollectionUtils.isNotEmpty(datas)){
#  logger.info(RunTimeLogUtil.toLog(LogObjectTypeEnum.SYSTEM,"consume","Get data platform Notify",null,null, "record"),JsonUtils.object2Json(datas));
#  // 起线程处理 资源变更通知
#  resourceHandle(datas);
#}
#} else
#  {
#    break;
#  }
#}
#  private void handlerConsumer(String kafkaIp, String kafkaPort) {
#  Properties props = new Properties();
#  props.setProperty("bootstrap.servers", kafkaIp + ":" + kafkaPort);
#  // key反序列化
#  props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
#  // value反序列化
#  props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
#  // 每个消费者都必须属于某一个消费组，所以必须指定group.id
#  props.put("group.id", "test");
#  // 构造消费者对象
#  deviceNoifyThreadExecutor.execute(()->{
#  KafkaConsumer<String, String> consumerObj = null;
#  // 指定多主题：
#  List<String> topics = CbdmOptUtil.stringToStringList(PropertiesUtil.getProperty("kafka.subscribe.topics"), ConstParamErrorCode.DEFAULT_SPLIT_KEY, false);
#  try {
#  consumerObj = new KafkaConsumer<>(props);
#  if(consumerObj != null) {
#  consumerObj.subscribe(topics);
#  resourceNotifyConsumer.setConsumer(consumerObj);
#  KafkaLinkCache.DEVICE_CONSUMER_MAP.put("kafkaComsumer", consumerObj);
#  resourceNotifyConsumer.onMessage();
#  }
#  } catch(Exception e) {
#  LogUtils.logError(RunTimeLogUtil.toErrorLog(ConstParamErrorCode.SYSTEM_CODE_FAIL + "", LogObjectTypeEnum.SYSTEM,"consume",
#  "resolve data platform notify error"),e);
#  }finally {
#  // 关闭
#  consumerObj.close();
#  }
#  });
#
#  //保存配置
#  KafkaLinkCache.kafkaConfigCache = kafkaIp + "_" + kafkaPort;
#}
